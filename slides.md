---
marp: true
theme: default
paginate: true
title: "От REST к MCP: как LLM меняют принципы проектирования API и архитектуры систем"
backgroundColor: white
lang: ru
---

<!-- _paginate: skip -->

# От REST к MCP: 
# как LLM меняют принципы проектирования API и архитектуры систем 
## Дмитрий Бондарев

---

# Введение. Постановка проблемы

---

<!-- header: Введение. Постановка проблемы -->

## Что такое API?

**API (application programming interface)** — это формально описанный способ, по которому одна программа обращается к другой: какие операции доступны, какие параметры и форматы данных нужны, какие ошибки возможны и что гарантирует система.

Иными словами, это **контракт** между «поставщиком» функций/данных и их «потребителем», независимый от внутренней реализации.

---

## Краткая историческая справка об API

- **≈80 лет назад (≈1945–1951):** 
    - Появились первые библиотеки подпрограмм и модульный подход.

- **≈57 лет назад (1968):** 
    - В инженерных текстах закрепился термин **API** (*application program interface*)

- **≈25 лет назад (≈2000):** 
    - Началась эпоха **веб-API**: компании стали открывать HTTP-интерфейсы для внешних разработчиков.


---

# Текущие способы формирования синхронного API
- REST (stateless)
    - overfetching/underfetching
- GraphQL
    - сложность кеширования и реализации вложенных запросов (N+1 problem)
- gRPC
    - жесткие контракты



---

# Таблица сравнения REST, gRPC, 

TBD


---

# во всех трех случаях клиент (программа) должен заранее знать API
ссылка на принцип из REST хетаос как то там


---

<!-- header: Что меняется с приходом LLM и как надо пересматривать принципы проектирония -->

# Что меняется с приходом LLM и как надо пересматривать принципы проектирония

---

Историческая справка с датой выхода GPT-3.5
2025 год - год Агентов

---


## Что такое Агент и отличие его от workflow

Неопределенность вычислительного DAG. Решение об используемых сервисах происходит в процессе выполнения.

Иначе это workflow и проблемы нет. Прибить гвоздями и жестко определить вызовы (но опять же может не быть знания какие методы использовать)
Но с другой стороны определен жестко LLM

### Какие же проблемы возникают при переходе на искользование LLM как consumer?

---

## Проблема MxN

![](attachments/MxN.png)

<!-- _footer: https://huggingface.co/learn/mcp-course/unit1/key-concepts -->

---

## Проблема что клиент не знает АПИ

// TODO перенести в раздел с MCP
- selfdiscovery
- стандартизированные точки входа

---

## Из вышеприведенного проблема с неопределенностью вычислительного DAG у Агентов 
Нельзя построить предсказуемый workflow

---

## Проблема что данные поступают постепенно

Решение через streamed протоколы


---


// TODO переписать
# Описание MCP спецификации и отличие его от REST и RPC
- Протокол MCP основан на JSON-RPC 2.0 и следует архитектуре Client-Host-Server
- MCP позиционируется как универсальный адаптер, подобно USB-C для AI-приложений, который обеспечивает стандартизированный способ подключения AI-моделей к различным источникам данных и инструментам.
- ключевые принципы
    - selfdiscovery -  AI-агент не должен заранее "знать" о возможностях сервера. Вместо этого, он может отправить запрос tools/list и получить машиночитаемый список доступных инструментов, их описания, входные и выходные параметры.
        - указать где то что такое агент
    - stateful - Передача контекста снижает накладные расходы, поскольку нет необходимости повторно отправлять информацию о состоянии в каждом запросе.
    - единый интерфейс
- добавить про поддержку авторизации

---

# Плюсы MCP

---

## M x N превращается в M + N
![](attachments/M+N.png)


<!-- _footer: https://huggingface.co/learn/mcp-course/unit1/key-concepts -->

---

# Проблемы которые MCP создает
## Метрики работы MCP

// TODO переписать
# Изменения в Системном дизайне
	- stateful-сессии усложняют Load Balancing и требуют реализации sticky-sessions

---


// TODO переписать
# Тестирование АПИ
	- Сложность тестирования мср. Нужна специальная программа (не постман)
	- Идея А/В теста методов MCP (как они захотят LLM-based агенты)

---

// TODO переписать
# Будущее API-дизайна

---

# Примеры кода на Go и Python

---


---


НИЖЕ ДАННЫЕ, которые еще не добавлены в структуру


Классический дизайн API
- Явная спецификация и статическая структура:
- Статический формат передачи данных: В REST параметры запроса могут передаваться в разных местах – путь URL, заголовки, query-параметры, тело (JSON/XML и т.д.), что приводит к множеству вариантов структурирования данных
- Ограниченная statefulness и контекст: Классические веб-API, особенно REST, обычно без сохранения состояния между вызовами
- Однонаправленное взаимодействие: В традиционных API коммуникация инициируется клиентом: запрос → ответ. Существуют способы серверной отправки (server-push) – вебсокеты, Server-Sent Events, GraphQL Subscriptions – но это дополнительные механизмы, не единообразные для всех сервисов
glama.ai. В общем случае, протокол HTTP/REST не подразумевает двунаправленной связи по умолчанию.

---

- Детерминированность и контроль: Вызовы API заранее запрограммированы. Разработчик точно формирует HTTP-запрос к известному эндпоинту с известными параметрами, и получает предсказуемый тип ответа. Ошибки вызова (неправильный путь, неверные параметры) – это ошибки кода клиента, которые разработчик отлавливает и исправляет. Таким образом, выполнение запроса максимально детерминировано (если код верен, запрос пройдет).

В итоге, классический подход к API сосредоточен на том, чтобы человеку-разработчику было удобно интегрироваться. Интерфейсы проектируются с учетом логики приложения и понятности для человека, а не для AI. Это накладывает ограничения: API-функции зачастую мелкоразделенные (каждый CRUD шаг отдельным вызовом), контекст между ними держит клиент, а для подключения новой системы нужен новый код или коннектор.

Отличие в принципах (вычленить MCP и связать с # Что меняется с приходом LLM и как надо пересматривать принципы проектирония)
MCP изначально спроектирован под взаимодействие AI-агента с вашим бэкендом. В модели MCP предполагается, что клиентом API выступает LLM, который сам решает, какие "инструменты" (tools) вызвать для выполнения задачи пользователя.

---

- Единый формат и строгий протокол: MCP – это конкретный протокол обмена сообщениями, фактически надстройка над JSON-RPC 2.0
en.wikipedia.org. Он строго регламентирует формат взаимодействия: вызовы идут как JSON-RPC запросы с единым схемой – метод, имя инструмента, JSON-объект аргументов, и стандартный формат ответа или ошибки

---

- Динамическое обнаружение возможностей: MCP вводит встроенные механизмы интроспекции. Клиент-LLM может в рантайме запросить у MCP-сервера список доступных инструментов и данных через стандартизованные методы (tools/list, resources/list, prompts/list

---

- Контекстность и stateful-взаимодействие: MCP поддерживает концепцию сеанса (session) и сохранение контекста между вызовами. Клиент и сервер MCP при установлении связи проводят lifecycle management – обмениваются информацией о поддерживаемых возможностях, устанавливают соединение
    - Фактически, протокол позволяет отслеживать открытые файлы, активные объекты или сессии, что приближает его по идее к Language Server Protocol из мира IDE
    - Например, MCP-сервер файловой системы может открыть файл и держать его содержимое как контекст, чтобы LLM затем запросил только изменения или выполнил поиск по уже загруженному тексту.
    - Как отметил один из авторов, MCP предназначен для stateful, context-driven взаимодействия: он отслеживает не только отдельные вызовы, но и ресурсы, шаблоны промптов и сам контекст сеанса

---

- Двунаправленная связь как базовая возможность: В MCP заложен канал обратной связи от сервера к клиенту как равноправная часть протокола. MCP-сервер может не только отвечать на запросы, но и инициировать сообщения к клиенту. В протоколе описаны нотификации и специальные методы, позволяющие, например, запросить у клиента-LLM продолжение генерации текста (LLM completion) или потребовать ввода от конечного пользователя (процесс, называемый elicitation в MCP). Сервер также может слать прогресс-уведомления и промежуточные результаты в процессе длительной операции Для классического API подобное потребовало бы отдельного вебсокета или долгого опроса, да и не стандартизовано, а здесь – часть протокола. 
    - Пример: MCP-сервер базы данных может выполнить долгий запрос и слать процент выполнения, или MCP-сервер задачи планирования может запросить у пользователя уточнение деталей через LLM, прежде чем продолжить. В результате взаимодействие становится интерактивным и адаптивным, что критично для сложных агентных сценариев.

---

- Крупнозернистые операции, соответствующие задачам пользователя. При проектировании MCP-инструментов рекомендуется ориентироваться на цельную задачу. Каждый инструмент MCP, по сути, представляет законченную функциональность, близкую к пользовательской задаче: “One tool, one human task”. Например, вместо трех отдельных REST-вызовов (POST /events, GET /conflicts, POST /invitations для создания события в календаре) может быть один MCP-инструмент calendar.create_event сразу со встроенной логикой проверки конфликтов и рассылки приглашений. 
    - В классических API обычно наблюдается более дробная гранулярность (в угоду повторному использованию и простоте отдельных методов). Но в контексте LLM-агента удобнее, когда один вызов решает целиком понятную человеку операцию, а не требует от AI orchestrator’а самостоятельно вызывать несколько эндпоинтов по очереди. Таким образом, подход к разбиению ответственности меняется: MCP-сервер может обернуть подкапотные несколько шагов (даже вызовы других REST API) в один инструмент, предоставив AI-агенту более декларативный и человекопонятный набор действий. Это облегчает цепочки рассуждений (chain-of-thought) модели и снижает риск, что она запутается в последовательности вызовов.

---

- Локальное исполнение и безопасность данных: MCP изначально рассчитан на сценарии, когда инструменты могут выполняться локально, рядом с AI-моделью (например, локально в среде IDE, как Claude Desktop, VS Code Copilot, и т.п.). Протокол поддерживает два режима: локальный (через STDIO, без сети) и удаленный (через HTTP + SSE). Локальный MCP-сервер – это просто запущенный процесс на той же машине, с которым AI-клиент общается через stdin/stdout. Это убирает накладные расходы (без HTTP оверхеда) и позволяет инструментам действовать с правами пользователя, запустившего AI.
    - Фактически, если вы запускаете AI-ассистента на своей машине, локальные MCP-инструменты могут иметь доступ к вашим файлам, выполнять команды в системе и т.д., подобно локальному приложению. Это открывает мощные возможности (например, инструмент, который действительно читает/записывает файлы напрямую, а не через API). В то же время, требует тщательного контроля – какие инструменты разрешены, какие операции они выполняют. В корпоративном бэкенде, скорее, MCP-сервисы будут удаленными с аутентификацией (например, через OAuth токены, как рекомендуется), но локальный вариант показывает философию: MCP сближает AI и систему максимально тесно. Классические же API почти всегда сетевые; даже если сервисы на одном хосте, они общаются через loopback HTTP, с соответствующими накладными расходами и ограничениями безопасности (нужно настраивать CORS, порты, и т.д. для доступа, что в MCP не требуется для локальных серверов).

---

- Единый протокол = возможность обучения модели: Стандартизация MCP дает долгосрочное преимущество – модель (LLM) может быть обучена эффективно пользоваться этим протоколом. Все инструменты разных сервисов выглядят для AI единообразно, поэтому будущие LLM можно специально тюнить под MCP-инструкции

---

// TODO поресечить про AI gateway
https://learn.microsoft.com/en-us/azure/api-management/export-rest-mcp-server
- Безопасность и новые риски: Поскольку MCP предполагает более автоматизированное использование API, появляются специфические риски. LLM, действующий как агент, может быть уязвим к prompt injection – когда злоумышленник хитрой фразой заставит модель вызвать нежелательный инструмент или передать конфиденциальные данные. Исследователи уже указали на потенциальные уязвимости MCP: объединение нескольких инструментов может привести к утечке данных, возможно создание "фальшивых" инструментов-двойников для перехвата управления, и т.д.
en.wikipedia.org
. Кроме того, доступ LLM к действиям требует строгого ограничения прав: нужно внедрять систему разрешений на инструменты (чтобы агент не получил больше данных, чем положено пользователю). В классическом API такие вопросы решаются правами пользователя и токенами, но там клиент – предсказуемый и не стремится обойти систему. В случае AI-агента же, приходится моделировать и ограничивать поведение самого клиента. Архитектурно возникают новые компоненты: например, AI Gateway (как упоминает Azure) для контроля MCP-трафика, политики допуска команд, мониторинга вызовов
learn.microsoft.com
. Это новый слой системного дизайна, обеспечивающий безопасное встраивание AI в инфраструктуру. Hardcore-бэкендерам это важно понимать: протокол MCP сильный инструмент, но требует дополнительных мер защиты, тестирования на злоупотребления и мониторинга, чтобы использование AI было надежным.


---

# Принципы системного дизайна

---

Оркестрация и взаимодействие компонентов тоже меняется. Если раньше фронтенд или BFF (Backend-for-Frontend) дирижировал последовательностью вызовов микросервисов, то с AI-агентом логика распределяется: часть передается самому LLM (который решает, в каком порядке вызывать инструменты), а часть – заложена в код MCP-серверов (где реализованы высокоуровневые действия). По сути, разработчики закладывают правила и ограничения, а подробный сценарий выполнения может получаться динамически. Это напоминает принципы Service-Oriented Architecture, но где “сервис” – интеллектуальный агент.
Чтобы такая схема работала правильно, архитектура должна предусмотреть наблюдаемость: tracing инструментальных вызовов, ограничение времени работы LLM-сессий, fallback-стратегии на случай, если агент не справится (например, если он много раз ошибся, возможно, передать задачу классической системе).

---

## Совместная жизнь классического API и MCP

---

## AI gateway

---

## Sticky sessions

---

Если классические API строились “для людей” – с учетом удобства разработки, читаемости, изоляции сервисов – то MCP строится “для машинного интеллекта”, чтобы он мог эффективно и безопасно работать с нашими сервисами. Подходы меняются кардинально: вводятся динамическое обнаружение возможностей, контекстность, интерактивность и стандартизация на уровне исполнения. Это ответ на требования новой эпохи, где большие модели станут полноправными потребителями и участниками ваших систем, а не просто выдающими текст над данными из одного запроса.

---

## Пример системного решения: 

Представьте корпоративную систему с микросервисами: один – управление задачами, другой – CRM, третий – база документов. Вы хотите, чтобы AI-бот в мессенджере помогал сотрудникам, например, собрать информацию о клиенте и завести задачу. С классическим подходом, пришлось бы прописать жесткую последовательность вызовов: запросить CRM, потом документы, потом создать задачу через API. С MCP можно внедрить AI-агента, который сам решит, что сначала надо вызвать crm.search_client, потом docs.find_related (например, инструмент поиска связанных документов), потом tasks.create. Ваша архитектура при этом содержит: MCP-сервер для CRM, MCP-сервер для документов и MCP-сервер для задач. Каждый из них общается с соответствующим микросервисом через привычные вызовы. AI-агент (в чат-боте) последовательно использует эти инструменты, а вы контролируете процесс через логи MCP и ограничения (например, инструменты выдают только данные, доступные конкретному пользователю, чье питание передан агенту). Таким образом, системный дизайн обогащается новой связностью: AI может действовать как умный оркестратор наравне с кодом, используя при этом все бэкенд-возможности стандартизованно.